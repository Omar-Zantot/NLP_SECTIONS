{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPU1YAp9K/w+V70PIRWYOak",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Omar-Zantot/NLP_SECTIONS/blob/main/Lab_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy upgrade and package installation"
      ],
      "metadata": {
        "id": "w5Gw4TMX3NaM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9MnkAj40lSx"
      },
      "outputs": [],
      "source": [
        "!pip install -U spacy==3.*\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy info\n",
        "!pip install -U scikit-learn scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t81VT9JboTzt"
      },
      "source": [
        "# Basic Bag-of-Words (BOW)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "u_EAof8njfHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1IVdG29wyJ7"
      },
      "source": [
        "## Plain frequency BOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fwfWQDVyJpY"
      },
      "source": [
        "# A corpus of sentences.\n",
        "corpus = [\n",
        "  \"Red Bull drops hint on F1 engine.\",\n",
        "  \"Honda exits F1, leaving F1 partner Red Bull.\",\n",
        "  \"Hamilton eyes record eighth F1 title.\",\n",
        "  \"Aston Martin announces sponsor.\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILvS020Zzm6F"
      },
      "source": [
        "We want to build a basic bag-of-words (BOW) representation of our corpus. Based on what you now know from the section, you can probably do this from scratch using dictionaries and lists (and maybe that's a good exercise). Fortunately, there are robust libraries which make it easy.\n",
        "\n",
        "We can use the scikit-learn **CountVectorizer** which takes a collection of text documents and creates a matrix of token counts."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "RCKiYbnb4C1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAphZMVPBX9P"
      },
      "source": [
        "The *fit_transform* method does two things:\n",
        "1. It creat a vocabulary dictionary from the corpus.\n",
        "2. It returns a matrix where each row represents a document and each column represents a token (i.e. term)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow = vectorizer.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "CUJJ57JE4KEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Bp1XNcF1FQ"
      },
      "source": [
        "We can take a look at the features and vocabulary dictionary. Notice the **CountVectorizer** took care of tokenization for us. It also removed punctuation and lower-cased everything."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQbqvLgVF8B7",
        "outputId": "1398480b-4041-40f3-d6a7-ed061d49804b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# View features (tokens).\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "# View vocabulary dictionary.\n",
        "vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['announces' 'aston' 'bull' 'drops' 'eighth' 'engine' 'exits' 'eyes' 'f1'\n",
            " 'hamilton' 'hint' 'honda' 'leaving' 'martin' 'on' 'partner' 'record'\n",
            " 'red' 'sponsor' 'title']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'red': 17,\n",
              " 'bull': 2,\n",
              " 'drops': 3,\n",
              " 'hint': 10,\n",
              " 'on': 14,\n",
              " 'f1': 8,\n",
              " 'engine': 5,\n",
              " 'honda': 11,\n",
              " 'exits': 6,\n",
              " 'leaving': 12,\n",
              " 'partner': 15,\n",
              " 'hamilton': 9,\n",
              " 'eyes': 7,\n",
              " 'record': 16,\n",
              " 'eighth': 4,\n",
              " 'title': 19,\n",
              " 'aston': 1,\n",
              " 'martin': 13,\n",
              " 'announces': 0,\n",
              " 'sponsor': 18}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifically, the **CountVectorizer** generates a sparse matrix using an efficient, compressed representation."
      ],
      "metadata": {
        "id": "vq28wt5J4eWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(bow))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4LsryhJ4i8w",
        "outputId": "c38e7b81-ae88-4103-e630-8035189f16a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse._csr.csr_matrix'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bywJ0XnGKPQ"
      },
      "source": [
        "If we look at the raw structure, we'll see tuples where the first element represents the document, and the second element represents a token ID. It's then followed by a count of that token. So in the second document (index 1), token 8 (\"f1\") occurs twice."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCF5Th4q4soH",
        "outputId": "6671818e-f880-4eac-bf16-fb31309e41c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 17)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 10)\t1\n",
            "  (0, 14)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 5)\t1\n",
            "  (1, 17)\t1\n",
            "  (1, 2)\t1\n",
            "  (1, 8)\t2\n",
            "  (1, 11)\t1\n",
            "  (1, 6)\t1\n",
            "  (1, 12)\t1\n",
            "  (1, 15)\t1\n",
            "  (2, 8)\t1\n",
            "  (2, 9)\t1\n",
            "  (2, 7)\t1\n",
            "  (2, 16)\t1\n",
            "  (2, 4)\t1\n",
            "  (2, 19)\t1\n",
            "  (3, 1)\t1\n",
            "  (3, 13)\t1\n",
            "  (3, 0)\t1\n",
            "  (3, 18)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv1N1Io2EyAb"
      },
      "source": [
        "Before we explore further, we want to make a few modifications.\n",
        "1. What if we want to use another tokenizer like spaCy's?\n",
        "2. Instead of frequency, what if we want to have a binary BOW?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRgIHkzUVJtk"
      },
      "source": [
        "## Binary BOW with custom tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tof1PBgqEy1D"
      },
      "source": [
        "**CountVectorizer** supports using a custom tokenizer. For every document, it will call your tokenizer and expect a list of tokens returned. We'll create a simple callback below which has spaCy tokenize and filter tokens, and then return them."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As usual, we start by importing spaCy and loading a statistical model.\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a tokenizer callback using spaCy under the hood. Here, we tokenize\n",
        "# the passed-in text and return the tokens, filtering out punctuation.\n",
        "def spacy_tokenizer(doc):\n",
        "  return [t.text for t in nlp(doc) if not t.is_punct]\n"
      ],
      "metadata": {
        "id": "7BvbJlFC5C1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drEe1Lv_OScv"
      },
      "source": [
        "This time, we instantiate **CountVectorizer** with our custom tokenizer (*spacy_tokenizer*), turn off case-folding, and also set the *binary* parameter to *True* so we simply get 1s and 0s marking token presence rather than token frequency."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True)\n",
        "bow = vectorizer.fit_transform(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6hQbG7J5O-G",
        "outputId": "fdc55276-d201-4a61-d044-74a57bdd1e0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jDKQkZUOysa"
      },
      "source": [
        "Looking at the resulting feature names and vocabulary dictionary, we can see our *spacy_tokenizer* being used. If you're not convinced, you can remove the punctuation filtering in our tokenizer and rerun the code."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out())\n",
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92Qr_B5g5Vsj",
        "outputId": "d51cffd5-3138-49a1-b195-50cbb642d9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Aston' 'Bull' 'F1' 'Hamilton' 'Honda' 'Martin' 'Red' 'announces' 'drops'\n",
            " 'eighth' 'engine' 'exits' 'eyes' 'hint' 'leaving' 'on' 'partner' 'record'\n",
            " 'sponsor' 'title']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Red': 6,\n",
              " 'Bull': 1,\n",
              " 'drops': 8,\n",
              " 'hint': 13,\n",
              " 'on': 15,\n",
              " 'F1': 2,\n",
              " 'engine': 10,\n",
              " 'Honda': 4,\n",
              " 'exits': 11,\n",
              " 'leaving': 14,\n",
              " 'partner': 16,\n",
              " 'Hamilton': 3,\n",
              " 'eyes': 12,\n",
              " 'record': 17,\n",
              " 'eighth': 9,\n",
              " 'title': 19,\n",
              " 'Aston': 0,\n",
              " 'Martin': 5,\n",
              " 'announces': 7,\n",
              " 'sponsor': 18}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFpQbdA-R3FI"
      },
      "source": [
        "To get a dense array representation of our sparse matrix, use *toarray*.\n",
        "\n",
        "We can also index and slice into the sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('A dense representation like we saw in the slides.')\n",
        "print(bow.toarray())\n",
        "print()\n",
        "print('Indexing and slicing.')\n",
        "print(bow[0])\n",
        "print()\n",
        "print(bow[0:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVnceSPm5gNW",
        "outputId": "546d6a74-70c7-4bc0-9f7f-7aae2c97fa99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A dense representation like we saw in the slides.\n",
            "[[0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            " [0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0]\n",
            " [0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1]\n",
            " [1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0]]\n",
            "\n",
            "Indexing and slicing.\n",
            "  (0, 6)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 13)\t1\n",
            "  (0, 15)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 10)\t1\n",
            "\n",
            "  (0, 6)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 13)\t1\n",
            "  (0, 15)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 10)\t1\n",
            "  (1, 6)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 2)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 11)\t1\n",
            "  (1, 14)\t1\n",
            "  (1, 16)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7e40ZAKhQmm"
      },
      "source": [
        "## Basic Bag-of-Words Exercises"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# EXERCISE: Create a spacy_tokenizer callback which takes a string and returns\n",
        "# a list of tokens (each token's text) with punctuation filtered out.\n",
        "#\n",
        "corpus = [\n",
        "  \"Students use their GPS-enabled cellphones to take birdview photographs of a land in order to find specific danger points such as rubbish heaps.\",\n",
        "  \"Teenagers are enthusiastic about taking aerial photograph in order to study their neighbourhood.\",\n",
        "  \"Aerial photography is a great way to identify terrestrial features that aren’t visible from the ground level, such as lake contours or river paths.\",\n",
        "  \"During the early days of digital SLRs, Canon was pretty much the undisputed leader in CMOS image sensor technology.\",\n",
        "  \"Syrian President Bashar al-Assad tells the US it will 'pay the price' if it strikes against Syria.\"\n",
        "]\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def spacy_tokenizer(doc):\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "a03Ce6MJ56fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# EXERCISE: Initialize a CountVectorizer object and set it to use\n",
        "# your spacy_tokenizer with lower-casing off and to create a binary BOW.\n",
        "#\n",
        "\n",
        "# Instantiate a CountVectorizer object called 'vectorizer'.\n",
        "\n",
        "\n",
        "# Create a binary BOW from the corpus using your CountVectorizer."
      ],
      "metadata": {
        "id": "QmpQc5YK7sWY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}